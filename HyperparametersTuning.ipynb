{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb48535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:46:32,593] A new study created in memory with name: no-name-0c21f2e7-6882-4644-8797-cda0214acac7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 1/3\n",
      "Trial 0, Fold 2/3\n",
      "Trial 0, Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:48:01,132] Trial 0 finished with value: 0.9870784935761331 and parameters: {'lr': 0.0001680720977739039, 'weight_decay': 4.123206532618725e-06, 'n_layers': 3, 'hidden_size_0': 64, 'dropout_0': 0.11742508365045984, 'hidden_size_1': 64, 'dropout_1': 0.10617534828874074, 'hidden_size_2': 64, 'dropout_2': 0.1545474901621302, 'batch_size': 64}. Best is trial 0 with value: 0.9870784935761331.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 completed with mean CV score: 0.9871\n",
      "Trial 1, Fold 1/3\n",
      "Trial 1, Fold 2/3\n",
      "Trial 1, Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:49:43,214] Trial 1 finished with value: 0.9863987210551296 and parameters: {'lr': 0.0002069830835864267, 'weight_decay': 5.418282319533238e-07, 'n_layers': 2, 'hidden_size_0': 64, 'dropout_0': 0.20990855298810754, 'hidden_size_1': 128, 'dropout_1': 0.2542703315240835, 'batch_size': 32}. Best is trial 0 with value: 0.9870784935761331.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 completed with mean CV score: 0.9864\n",
      "Trial 2, Fold 1/3\n",
      "Trial 2, Fold 2/3\n",
      "Trial 2, Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:51:26,021] Trial 2 finished with value: 0.9874861572535991 and parameters: {'lr': 0.00023215521736070897, 'weight_decay': 1.9485671251272545e-07, 'n_layers': 2, 'hidden_size_0': 128, 'dropout_0': 0.19138413075201122, 'hidden_size_1': 128, 'dropout_1': 0.13661147045343366, 'batch_size': 32}. Best is trial 2 with value: 0.9874861572535991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 completed with mean CV score: 0.9875\n",
      "Trial 3, Fold 1/3\n",
      "Trial 3, Fold 2/3\n",
      "Trial 3, Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:52:44,967] Trial 3 finished with value: 0.9863981659575295 and parameters: {'lr': 0.00035274870932829836, 'weight_decay': 2.75206968507905e-07, 'n_layers': 3, 'hidden_size_0': 256, 'dropout_0': 0.15545633665765812, 'hidden_size_1': 64, 'dropout_1': 0.3684482051282947, 'hidden_size_2': 128, 'dropout_2': 0.15879485872574356, 'batch_size': 64}. Best is trial 2 with value: 0.9874861572535991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 completed with mean CV score: 0.9864\n",
      "Trial 4, Fold 1/3\n",
      "Trial 4, Fold 2/3\n",
      "Trial 4, Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:53:55,091] Trial 4 finished with value: 0.9868069953399555 and parameters: {'lr': 0.00017139851136597887, 'weight_decay': 2.890772174372671e-07, 'n_layers': 3, 'hidden_size_0': 256, 'dropout_0': 0.1422772674924288, 'hidden_size_1': 256, 'dropout_1': 0.33167343078899725, 'hidden_size_2': 256, 'dropout_2': 0.3120572031542852, 'batch_size': 64}. Best is trial 2 with value: 0.9874861572535991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 completed with mean CV score: 0.9868\n",
      "Trial 5, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:54:00,467] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:54:04,005] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:54:16,091] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:54:21,555] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:54:27,294] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:54:32,599] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:54:36,043] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:54:39,770] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13, Fold 1/3\n",
      "Trial 13, Fold 2/3\n",
      "Trial 13, Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:55:41,856] Trial 13 finished with value: 0.984765623915825 and parameters: {'lr': 0.00024657586698012215, 'weight_decay': 1.0372483495581072e-06, 'n_layers': 3, 'hidden_size_0': 128, 'dropout_0': 0.1834316620958786, 'hidden_size_1': 128, 'dropout_1': 0.210846140918622, 'hidden_size_2': 128, 'dropout_2': 0.19927385254204452, 'batch_size': 64}. Best is trial 2 with value: 0.9874861572535991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 completed with mean CV score: 0.9848\n",
      "Trial 14, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:55:48,293] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:55:52,097] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:55:55,769] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:56:00,965] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:56:12,532] Trial 18 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19, Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 20:56:16,065] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Study statistics:\n",
      "  Number of finished trials: 20\n",
      "  Number of pruned trials: 14\n",
      "\n",
      "Best trial:\n",
      "  Value (Mean CV Accuracy): 0.9875\n",
      "  Params:\n",
      "    lr: 0.00023215521736070897\n",
      "    weight_decay: 1.9485671251272545e-07\n",
      "    n_layers: 2\n",
      "    hidden_size_0: 128\n",
      "    dropout_0: 0.19138413075201122\n",
      "    hidden_size_1: 128\n",
      "    dropout_1: 0.13661147045343366\n",
      "    batch_size: 32\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "# Fixed random seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_all_seeds(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_data():\n",
    "    X_train = pd.read_csv('UCI_HAR_Dataset/train/X_train.txt', sep='\\\\s+', header=None)\n",
    "    y_train = pd.read_csv('UCI_HAR_Dataset/train/y_train.txt', sep='\\\\s+', header=None, names=['Activity'])\n",
    "    X_test = pd.read_csv('UCI_HAR_Dataset/test/X_test.txt', sep='\\\\s+', header=None)\n",
    "    y_test = pd.read_csv('UCI_HAR_Dataset/test/y_test.txt', sep='\\\\s+', header=None, names=['Activity'])\n",
    "\n",
    "    features = pd.read_csv('UCI_HAR_Dataset/features.txt', sep='\\\\s+', header=None)\n",
    "    feature_names = features[1].tolist()\n",
    "    \n",
    "    X_train.columns = feature_names\n",
    "    X_test.columns = feature_names\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "activity_labels = {\n",
    "    1: 'WALKING',\n",
    "    2: 'WALKING_UPSTAIRS',\n",
    "    3: 'WALKING_DOWNSTAIRS',\n",
    "    4: 'SITTING',\n",
    "    5: 'STANDING',\n",
    "    6: 'LAYING'\n",
    "}\n",
    "\n",
    "y_train['Activity'] = y_train['Activity'].map(activity_labels)\n",
    "y_test['Activity'] = y_test['Activity'].map(activity_labels)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train['Activity'])\n",
    "y_test_encoded = encoder.transform(y_test['Activity'])\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.LongTensor(y_train_encoded)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.LongTensor(y_test_encoded)\n",
    "\n",
    "class HAR_MLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_sizes, dropout_rates):\n",
    "        super(HAR_MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for i, (h_size, drop_rate) in enumerate(zip(hidden_sizes, dropout_rates)):\n",
    "            layers.append(nn.Linear(prev_size, h_size))\n",
    "            layers.append(nn.BatchNorm1d(h_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(drop_rate))\n",
    "            prev_size = h_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    eval_loss = running_loss / len(data_loader)\n",
    "    eval_acc = correct / total\n",
    "    return eval_loss, eval_acc, np.array(all_preds), np.array(all_targets)\n",
    "\n",
    "def objective(trial):\n",
    "    trial_seed = SEED + trial.number\n",
    "    set_all_seeds(trial_seed)\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 0.0001, 0.0004, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-7, 5e-6, log=True)\n",
    "    \n",
    "    n_layers = trial.suggest_int('n_layers', 2, 3)\n",
    "    hidden_sizes = []\n",
    "    dropout_rates = []\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        hidden_sizes.append(trial.suggest_categorical(f'hidden_size_{i}', [64, 128, 256]))\n",
    "        dropout_rates.append(trial.suggest_float(f'dropout_{i}', 0.1, 0.4))\n",
    "    \n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    \n",
    "    k_folds = 3\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=trial_seed)\n",
    "    \n",
    "    cv_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_tensor)):\n",
    "        print(f\"Trial {trial.number}, Fold {fold+1}/{k_folds}\")\n",
    "        \n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "        \n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(trial_seed + fold)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size,\n",
    "            sampler=train_sampler,\n",
    "            worker_init_fn=lambda worker_id: np.random.seed(trial_seed + fold + worker_id)\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=val_sampler\n",
    "        )\n",
    "        \n",
    "        model = HAR_MLP(\n",
    "            input_size=X_train.shape[1],\n",
    "            num_classes=len(np.unique(y_train_encoded)),\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            dropout_rates=dropout_rates\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        max_epochs = 100\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    break\n",
    "\n",
    "            global_step = fold * max_epochs + epoch\n",
    "            trial.report(val_acc, global_step)\n",
    "            \n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        cv_scores.append(best_val_acc)\n",
    "    \n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    print(f\"Trial {trial.number} completed with mean CV score: {mean_cv_score:.4f}\")\n",
    "    \n",
    "    return mean_cv_score\n",
    "\n",
    "def optimize_hyperparameters(n_trials=20):\n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    \n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        pruner=pruner,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"  Completed trials: {len(study.trials)}\")\n",
    "    print(f\"  Pruned trials: {len([t for t in study.trials if t.state == TrialState.PRUNED])}\")\n",
    "    \n",
    "    print(\"\\nBest trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  Best accuracy: {trial.value:.4f}\")\n",
    "    print(\"  Parameters:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    return study, study.best_params\n",
    "\n",
    "study, best_params = optimize_hyperparameters(n_trials=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
